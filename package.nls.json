{
  "displayName": "NexoAgent",
  "description": "Production-grade multi-agent AI coding assistant. Plan → Code → Review autonomously.",
  "command.runAgent": "NexoAI: Run Agent",
  "command.explainCode": "NexoAI: Explain Selected Code",
  "command.fixCode": "NexoAI: Fix Selected Code",
  "command.refactorCode": "NexoAI: Refactor Selected Code",
  "command.addTests": "NexoAI: Generate Tests for Selected Code",
  "command.addDocs": "NexoAI: Add Documentation to Selected Code",
  "command.fixErrors": "NexoAI: Fix Errors in Current File",
  "command.createApp": "NexoAI: Create Full App",
  "command.undo": "NexoAI: Undo Last Changes",
  "command.openSettings": "NexoAI: Open Settings",
  "command.focusChat": "NexoAI: Focus Chat Panel",
  "command.setApiKey": "NexoAI: Set API Key",
  "command.deleteApiKey": "NexoAI: Delete API Key",
  "command.resetTokens": "NexoAI: Reset Token Counter",
  "command.setOpenRouterApiKey": "NexoAI: Set OpenRouter API Key",
  "command.deleteOpenRouterApiKey": "NexoAI: Delete OpenRouter API Key",
  "command.switchProvider": "NexoAI: Switch Model Provider",
  "config.title": "NexoAgent",
  "config.provider.desc": "Model provider to use. 'nvidia' uses the NVIDIA API (build.nvidia.com). 'openrouter' uses OpenRouter (openrouter.ai) giving access to Claude, GPT-4, Gemini, Llama, and 200+ models.",
  "config.baseUrl.desc": "API base URL.",
  "config.model.desc": "The model to use when provider is 'nvidia'. Browse available models at build.nvidia.com.",
  "config.openRouterModel.desc": "The model to use when provider is 'openrouter'. Browse available models at [openrouter.ai/models](https://openrouter.ai/models).",
  "config.openRouterBaseUrl.desc": "OpenRouter API base URL. Only change this if using a self-hosted proxy.",
  "config.temperature.desc": "Sampling temperature (lower = more deterministic).",
  "config.topP.desc": "Top-p nucleus sampling.",
  "config.maxTokens.desc": "Maximum tokens per model response.",
  "config.maxIterations.desc": "Maximum tool-use iterations per agent run.",
  "config.autoApply.desc": "Automatically apply file changes without confirmation.",
  "config.contextLines.desc": "Number of lines of context to include when reading files.",
  "config.maxFileSize.desc": "Maximum file size (bytes) to read in full.",
  "config.commandTimeout.desc": "Timeout for shell commands (milliseconds).",
  "config.telemetry.desc": "Enable anonymous usage telemetry (disabled by default).",
  "config.styleEnforcement.desc": "Enable Claude-style quality pipeline (Generate → Critic → Rewrite → Verify) for code generation.",
  "config.candidateCount.desc": "Number of candidate outputs to generate per request (higher = better quality but slower). Default: 3.",
  "config.styleThreshold.desc": "Minimum combined style score (0–100) before triggering an automatic rewrite pass. Default: 70.",
  "config.codeTemperature.desc": "Temperature override for code generation (0.0–0.15 for deterministic output). Default: 0.05.",
  "config.enableSubAgents.desc": "Enable dynamic sub-agent decomposition for complex enterprise tasks. When enabled, complex goals are automatically split into parallel sub-tasks handled by specialized domain agents.",
  "config.maxSubAgents.desc": "Maximum number of sub-agents to run concurrently during task decomposition. Default: 4.",
  "config.subAgentTimeout.desc": "Per sub-agent timeout in milliseconds. Default: 120000 (2 minutes).",
  "config.complexityThreshold.desc": "Minimum complexity score (0–100) before triggering the dynamic sub-agent pipeline. Lower = more tasks use sub-agents. Default: 50.",
  "config.enableMemory.desc": "Enable persistent conversation memory. Stores conversation history, facts, and summaries to disk to prevent context rot across sessions.",
  "config.enableRAG.desc": "Enable RAG (Retrieval-Augmented Generation). Indexes workspace files and retrieves relevant code chunks for each query, reducing hallucination.",
  "config.thinkMode.desc": "Extended thinking mode. 'off' = disabled, 'auto' = toggle via toolbar button, 'always' = always active. Think mode makes the model reason step-by-step before answering.",
  "config.thinkBudget.desc": "Maximum reasoning tokens for think mode. Higher values allow deeper reasoning but increase latency and cost. Default: 2048.",
  "config.enableMCP.desc": "Enable MCP (Model Context Protocol) servers. Connects to external tool servers defined in .nexo-ai/mcp.json or settings. Extends the agent with custom tools.",
  "config.mcpServers.desc": "MCP server configurations. Each key is a server name, value is an object with 'command', 'args', and optional 'env'. Example: { \"filesystem\": { \"command\": \"npx\", \"args\": [\"-y\", \"@anthropic/mcp-filesystem\"] } }",
  "view.chat.title": "Chat",
  "viewContainer.title": "NexoAgent"
}
